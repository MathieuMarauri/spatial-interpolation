---
title: "Spatial interpolation in R"
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    theme: cerulean
    highlight: tango
    css: style.css
bibliography: biblio.bib
biblio-style: "apalike"
link-citations: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "all",
      } 
  }
});
</script>

<!-- https://groups.google.com/forum/#!topic/knitr/OMvT03PtGPM --> 

```{r initial_chunk, echo = FALSE, warning = FALSE, message = FALSE}
library("knitr")
opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, vcache = FALSE, fig.align = 'center', dpi = 300, out.width = '75%')
```

```{r set-up}
library("cowplot") # Export plots
library("data.table") # Fast dataset manipulation
import::from("fields", "rdist") # rdist function
library("geoR") # Analysis of geo-statistical data
library("ggplot2") # Data visualisations using the Grammar of Graphics
library("ggrepel") # Text labels
library("gstat") # Geo-statistical modelling
library("magrittr") # Pipe operators
library("progress") # Progress bars
library("stringi") # String manipulation
source("../src/helpers.R") # Load custom functions

# Set default ggplot theme
theme_set(
  theme_light(
  base_size = 12
  ) +
  theme(
    text = element_text(family = "Gibson", colour = "gray10"),
    panel.border = element_blank(),
    axis.line = element_line(colour = "gray50", size = .5),
    # axis.ticks = element_blank(),
    strip.background = element_rect(colour = "gray50", fill = "transparent", size = .7),
    strip.text.x = element_text(colour = "gray10"),
    strip.text.y = element_text(colour = "gray10"),
    legend.key.size = unit(.7, "cm")
  )
)

# Set default scales
scale_colour_continuous <- function(...) ggplot2::scale_colour_viridis_c(..., option = "viridis")
scale_colour_discrete <- function(...) ggplot2::scale_colour_viridis_d(..., option = "viridis")
scale_fill_continuous <- function(...) ggplot2::scale_fill_viridis_c(..., option = "viridis")
scale_fill_discrete <- function(...) ggplot2::scale_fill_viridis_d(..., option = "viridis")

options(geoR.messages = FALSE)
```

# Introduction

Spatial interpolation is the process of using points with known values to estimate values at other points ([QGIS definition](https://docs.qgis.org/3.10/en/docs/gentle_gis_introduction/spatial_analysis_interpolation.html)). When asked a question like _What is the air pollutants concentration over the entire city?_ and you only have access to measures at some observation stations, the answer is *spatial interpolation*. 

There are many use cases, depth maps, concentration of mineral resources in the soil of a region, air pollution, elevation map, ...The common point is that the information of interest (altitude, concentration, ...) is known at sample points and one wants to know it at every point of a specific region. 

There exists different solutions to this spatial interpolation problem. They often consist on taking a weighted average of the sample points. In this document 3 methods are presented:

* Inverse Distance Weighting (IDW), 
* Triangular Irregular Networks (TIN), 
* Kriging

#### The mathematical problem

Before going into detail into the way these methods work, let's give a proper mathematical definition of the problem.

Let's say you have $n$ measures $z_1, \dots, z_n$ of some variable $Z$ at $n$ points in some region of space $R \subset \mathbb{R}^2$. You want to know the value of $Z$ at every points in that region $R$. $Z$ can be the depth and $R$ a coastal region for example. 

The ultimate goal is to define a function that for each point $(x, y) \in \mathbb{R}^2$ associates a value $z \in \mathbb{R}$.

$$
(x, y) \in \mathbb{R}^2 \rightarrow z \in \mathbb{R}
$$
Finding this function is what spatial interpolation is about. To do so the only input data at your disposal are the $n$ estimates at sample points $s_1, \dots , s_n$, $s_i \in \mathbb{R}^2, i = 1 \dots n$. 

Different methods exist [see @overview] and most of them have the same general formula. The prediction $\widehat{Z_{s_0}}$ for a point $s_0 \in \mathbb{R}^2$ is given by the following relation:

<a id="the-equation"></a>
$$
\widehat{Z_{s_0}} = \sum_{i = 1}^{n}\lambda_i Z_{s_i}
$$

The predictions are weighted average of the values observed at sample points $s_1, \dots , s_n$. The way the weights $\lambda_1, \dots , \lambda_n$ are defined is what make each method different from the others. However weights are defined, every method follows the [Tobler's first law of geography](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography) that states that _"everything is related to everything else, but near things are more related than distant things"_.

#### The application

The methods will be explained theoretically and code will be provided to apply them on toy data. Although the aim of this document is not to compare each method, as choosing the best one depends on the data, the same example data will be used throughout the document. It consists of a gaussian random field simulated on a $[0, 100] \times [0, 100]$ grid with the `geoR::grf()` function. A random sample of 100 points are selected and will be used as the basis for the spatial interpolation. 

<a id="grf"></a>
```{r grid, out.width="100%"}
simulated_grid <- readRDS("../data/grf_spheric_01x01.rds")
set.seed(123)
sample_points <- simulated_grid[sample(1:10201, 100), ]
plot_map(simulated_grid, "The 100 sample points on the true map") +
  geom_point(
    data = sample_points,
    mapping = aes(x = x, y = y),
    size = 1
  )
```

Besides this pretty simple scenario, additional use cases will be explored for each methods. 

# Inverse Distance Weighting

The first method that we'll explore is IDW. It uses [this equation](#the-equation) to make the predictions and the weights are defined using, you guessed it, the inverse of the distance to sample points.

After explaining the how, 2 applications with R code are provided, a really basic one to illustrate the theory and a more challenging one, where barriers are added, but we'll come to that later. 

## How it works

First let's dive into the method. The weights $\lambda_1, \dots , \lambda_n$ are defined by taking the inverse of the distance between the point to predict, $s_0$, and the sample points $s_1, \dots , s_n$, at a specific power $p$. This method directly considers that the closer a point is to another the more alike they are. If $dist(a, b) = ||a-b||^2$ is the euclidean distance between points $a, b \in \mathbb{R}^2$ then for $s_0 \in R$, $\forall i = 1\dots n$ and $p \in \mathbb{N}$, the weights $\lambda_1, \dots , \lambda_n$ are defined with the following equation.

<a id="idw-weight"></a>
$$
\lambda_i = \frac{\left(dist(s_0, s_i)^p\right)^{-1}}{\left(\sum_{j=1}^{n}dist(s_0, s_j)^p\right)^{-1}}
$$

The weights are normalized by the sum of all weights so they sum to 1. This also removes the dependence from the unit (metres or km). The parameter $p$ defines the rate at which the influence of a point to another decreases. The higher $p$ is the fastest the decrease of influence is.

```{r power}
x_seq <- seq(0, 10, length.out = 11)
data <- data.table(
    x = rep(x_seq, 5),
    y = c(1 / x_seq^0, 1 / x_seq^1, 1 / x_seq^2, 1 / x_seq^3, 1 / x_seq^4),
    color = rep(as.character(0:4), each = 11)
  ) %>% 
  .[x == 3, label := paste("p =", color), by = color]
ggplot(
  data = data, 
  mapping = aes(x = x, y = y, color = color, group = color)
) + 
  geom_line(
    show.legend = FALSE
  ) + 
  scale_x_continuous(
    breaks = seq(0, 10, length.out = 6)
  ) + 
  geom_label_repel(
    mapping = aes(label = label),
    nudge_x = 1,
    show.legend = FALSE
  ) + 
  labs(
    x = "Distance",
    y = latex2exp::TeX("$1 / x^p$"),
    title = "Comparison of different power orders"
  )
```

$p$ is an hyper-parameter of the IDW method and the most adequate value should be selected. One solution to this problem will be presented in a [dedicated section](#p-choice) later.

## A first application

First let's apply IDW on our [simulated random field](#grf). Recall we have 100 points and we want to predict the value of the variable of interest $Z$ for every point of the grid $[0, 100] \times [0, 100]$.

Building the interpolated map means making a prediction for each point in the grid. Following formula 2 and 3 we need to:

1. compute the distance matrix between each point and the sample points
2. use these distances to build the weights
3. build our prediction according to equation 2 $\widehat{Z_{s_0}} = \sum_{i = 1}^{n}\lambda_i Z_{s_i}$

### Step by step code {#idw-step-by-step}

The process to make predictions on the grid is presented in the following code. Of course specific functions exist to do just that but let's break it piece by piece first. 

The order used here is 5, we will see later on how this value was selected.

```{r idw_base, echo = TRUE}
# Distance matrix
distance <- rdist(simulated_grid[, c("x", "y")], sample_points[, c("x", "y")])

# Weights matrix
# Take the inverse of every element of the distance matrix
weights <- apply(
  X = distance,
  MARGIN = 1:2,
  FUN = function(x) 1 / x^5
)
# Divide (FUN = "/") the weights by the row sum (STATS = rowSums(weights)) to every element of the
# weights matrix (x = weights) row wise (MARGIN = 1)
weights <- sweep(
  x = weights,
  MARGIN = 1,
  STATS = rowSums(weights),
  FUN = "/"
)

# Multiply the weights matrix with the z value of the sample points
pred <- weights %*% sample_points$z
```

Those lines of code are the only ones needed to have an estimated $Z$ value at each location of the grid $[0, 100] \times [0, 100]$. The result, `pred`, is a table with $101 \times 101 = 10201$ values, one for each point of the grid. The following figure displays the predicted map alongside the true data.

```{r idw_pred_map}
type_lables <- c("IDW map", "Real map")
names(type_lables) <- c("pred", "true")
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "true" = simulated_grid$z) %>%
  merge(
    y = sample_points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
plot_map("Prediction with IDW and real data") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = type_lables)
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

The map is globally the same (if you're not too exigent) but much smoother than the real one. It displays some "bubbles" around the sample points which is of course coherent with the fact that each point has a strong influence only on its immediate neighbourhood (recall $p$ was set to 5).

### How to chose $p$? {#p-choice}

The order $p$ was set to 5. Why not 3 or 6 or even 10? The choice of $p$ depends on the data you have and one should always try to find the most appropriate value. One way to do so is to evaluate the error of prediction using a leave-one-out estimation. For each point in the sample a prediction is made using all the other points. The estimation of the error is then the average error observed. 

More specifically if we have $n$ points $s_1, \dots, s_n$, the leave-one-out error estimation is computed as follows:

$$
\begin{equation}
\begin{split}
E_{LOO} &= \sqrt{\frac{1}{n} \sum_{i=1}^{n}(Z_{s_i} - \widehat{Z_{s_i}})^2} \\
\widehat{Z_{s_i}} &= \sum_{j=1, j \ne i}^{n}\lambda_j Z_{s_j}
\end{split}
\end{equation}
$$

The weights are computed as defined in [equation 3](#idw-weight). This error is computed for different values of $p$ and the value of $p$ that minimizes the error is to be chosen. 

The following piece of code illustrates this method. The error is estimated for $p = 1\dots 20$.

```{r loo, echo = TRUE}
rmse_errors <- sapply(
  X = 1:20, # Values of p
  FUN = function(p) { # Compute the RMSE estimation for each value of p
    pred_sample <- numeric(length = 100)
    for (i in length(pred_sample)) {
      # Predict point i and remove it from sample points
      distance <- rdist(sample_points[i, c("x", "y")], sample_points[-i, c("x", "y")])
      weights <- apply(
        X = distance,
        MARGIN = 1:2,
        FUN = function(x) 1 / x^p
      )
      weights <- sweep(
        x = weights,
        MARGIN = 1,
        STATS = rowSums(weights),
        FUN = "/"
      )
      pred_sample[i] <- weights %*% sample_points[-i, ]$z %>% as.numeric()
    }
    return(sqrt(sum((pred_sample - sample_points$z)^2) / nrow(sample_points)))
  }
)
```


```{r loo_plot}
ggplot(
  data = data.frame(x = 1:20, y = rmse_errors),
  mapping = aes(x = x, y = y)
) +
  geom_line() +
  labs(
    x = "Power order",
    y = "Leave-one-out RMSE",
    title = "RMSE for different orders p"
  )
```


The RMSE (estimated with leave-one-out) first decreases a lot and reach a plateau around 5. The actual best value is `r which.min(rmse_errors)` but from this plot one can see that choosing 5 or 20 does not make a huge difference.


### Of course an idw function exists

The package `gstat` has an `idw()` function that does exactly what we did. It takes 5 arguments: 

* `formula` a formula giving the name of the target variable. For IDW the right hand side is 1, meaning no independent variables.
* `locations` a formula giving the names of the coordinates variables.
* `data` the name of the dataset containing the variables defined in `formula` and `locations`.
* `newdata` a table with the coordinates where to make predictions.
* `idp` the power order to use.

```{r idw, echo = TRUE, eval = FALSE}
pred_gstat <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = sample_points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5
)
```

```{r idw2, echo = FALSE}
pred_gstat <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = sample_points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5,
  debug.level = 0
)
```

As a sanity check let's compare the map obtained from the code of the [previous section](#idw-step-by-step) and the one with `gstat::idw()`.

```{r idw_sanity}
names(pred_gstat) <- c("x", "y", "z", "var")
data.table(simulated_grid[, c("x", "y")], "idw_base" = pred[, 1], "idw_gstat" = pred_gstat$z) %>%
  merge(
    y = sample_points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(idw_base), idw_base := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Step by step and gstat IDW produce the same maps") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("idw_base" = "Step by step", "idw_gstat" = "gstat::idw()"))) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

## Adding some spatial constraints

```{r barriers_data}
barriers <- readRDS("../data/barriers.rds")
points <- readRDS("../data/points.rds")
```

So far we considered that each sample point influences every point of the region (even if this influence could be small depending on the distance). In the presence of barriers (obstacles blocking the underlying physical process, causing linear discontinuity in the surface) some points can have no influence at all. When interpolated a value in a coastal region, we often encounter such barriers, (islands, port docks, ...).

To account for the presence of such barriers the interpolation algorithm must be modified. One can no longer use euclidean distance between all points to make a prediction. A first solution is to remove sample points that are not in the _line of sight_ of the point to predict.

```{r line-of-sight, fig.cap = "Source = [ArcGIS blog on esri.com](https://www.esri.com/arcgis-blog/products/analytics/analytics/interpolation-in-the-presence-of-barriers/)"}
include_graphics("images/idw_barriers.png")
```

Whenever a sample point is not in line of sight of the point to predict then its value is not used (its weight is set to 0) for the interpolation.

Another method, called Inverse Path Distance Weighting (IPDW) could also be used [see @ipdw]. It creates a cost surface and shortest path to define the distance between points. [This vignette](https://cran.r-project.org/web/packages/ipdw/vignettes/ipdw2.html), from the `ipdw` package [see @ipdw-package] explained the process.

A different toy data is used in this section.

```{r barrier_data_plot}
ggplot() + 
  geom_sf(data = barriers) + 
  geom_point(
    data = points,
    mapping = aes(x = x, y = y)
  ) + 
  geom_rect(
    mapping = aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1),
    color = "grey20",
    fill = NA
  ) + 
  # expand_limits(
  #     x = c(-0.1, 1.1),
  #     y = c(-0.1, 1.1)
  #   ) +
  labs(
    title = "Region to interpolate with sample points and barriers"
  ) + 
  theme(
    panel.grid = element_blank(),
    axis.line = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  )
```

We detail here how the first solution could be implemented. The idea is to update the weights matrix (as defined in [this section](#idw-step-by-step)) by setting to 0 weights between points that are not in _line of sight_. This piece of code is quite slow and only serves as explanatory purpose.

```{r constained_idw, echo = TRUE, eval = FALSE}
# First compute the distance matrix between all points of the grid and all sample points
distance <- rdist(simulated_grid[, c("x", "y")], points[, c("x", "y")])

# Define the power order
p <- 5

# Take the inverse of every element
weights <- apply(
  X = distance,
  MARGIN = 1:2,
  FUN = function(x) 1 / x^p
)

#### The weights update

# Remove connection that are not in "line of sight". For each pair of point build a line and check
# wether it crosses a barrier
to_remove <- matrix(FALSE, nrow = nrow(weights), ncol = ncol(weights))
for (i in seq_len(nrow(simulated_grid))) {
  for (j in seq_len(nrow(points))) {
    # Build the line between a sample point and the point to predict
    to_remove[i, j] <- rbind(as.numeric(points[j, c("x", "y")]), as.numeric(simulated_grid[i, c("x", "y")])) %>%
      st_linestring() %>%
      # Check whether it crosses a barrier
      st_intersects(barriers) %>%
      lengths() > 0
  }
}
weights[to_remove] <- 0

####

# Divide (FUN = "/") the weights by the row sum (STATS = rowSums(weights)) to every element of the
# weights matrix (x = weights) row wise (MARGIN = 1)
weights <- sweep(
  x = weights,
  MARGIN = 1,
  STATS = rowSums(weights),
  FUN = "/"
)

# Multiply the weights matrix with the z value of the sample points
pred <- weights %*% points$z
```

The resulting map is given in the following figure along with the map obained with regular IDW. 

```{r idw_constained_plot}
# Interpolation with IDW
pred_idw <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5
)
names(pred_idw) <- c("x", "y", "z", "var")
# Load results from constained IDW
pred <- readRDS("../output/pred_idw_barriers.rds")
# Plot the result alongside the IDW prediction
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "idw" = pred_idw$z) %>%
  merge(
    y = points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Prediction with IDW and constrained IDW") +
  geom_sf(data = barriers) +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("pred" = "Constrained IDW", "idw" = "Regular IDW"))
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

The impact of taking the barrier into account clearly appears. The interpolated values south of the bottom left barrier have lower values in the constrained IDW case. The barrier blocks the influence of the point north of that barrier to go beyond it. The difference _Constrained - Regular IDW_ is plotted on this figure.

```{r barrier_reg_plot}
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "idw" = pred_idw$z) %>%
  merge(
    y = points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := pred - idw] %>%
  plot_map("Difference between the constrained IDW and the regular one") + 
  geom_sf(data = barriers)
```

We clearly see that the barriers prevent the high values located in the north-west part of the region to extend beyond the barriers. 

# Triangulated irregular network

[Delaunay traingulation](http://www.cs.uu.nl/geobook/interpolation.pdf)

# Kriging

Kriging is another method used to perform spatial interpolation. As IDW it uses the  [second formula](#the-equation) $\widehat{Z_{s_0}} = \sum_{i = 1}^{n}\lambda_i Z_{S_i}$ to make predictions. The way the weights are defined is what makes this methods specific. 

Before explaining the method we need to define some concepts: what a random field is, how stationarity is defined and what a variogram is. 

#### Random field

A random field, or random function, is simply a set of random variables. We consider each value $z_{s_1}, \dots, z_{s_n}$ to be observations of the random variables $Z_{s_1}, \dots, Z_{s_n}$. $Z = (Z_{s_1}, \dots, Z_{s_n})$ is a random function, or random field. $s$ represents a position in space. 

#### Stationarity

There are 3 kinds of stationarity: the strict stationarity, the second-order stationarity and the intrinsic stationarity. Refer to [@stationarity] for more information. [This document](https://informatique-mia.inra.fr/sites/informatique-mia.inra.fr.biosp-d7/files/oldfiles/geostat_janv12.pdf) (in French) also provides a nice definition of the concepts that are used in Kriging.

##### The strict stationarity

It states that the joint distribution of $Z_{s_i}$ is the same as the one of $Z_{s_i + h}$ where $h$ indicates a translation. It does not require the first moment nor the second one to exist.

##### The second-order stationarity

It is weaker than the first one. $Z$ is said to be second-order stationary if the first 2 moments exists and are invariant by translation.

Therefore for all $s$, $s'$ and $h$ we have:

$$
\begin{gathered}
Cov(Z_s, Z_{s'}) = Cov(Z_{s+h}, Z_{s'+h}) \\
\mathbb{E}[Z_s] = \mathbb{E}[Z_{s+h}]
\end{gathered}
$$

In other words weak stationarity is defined by:

$$
\begin{gathered}
Cov(Z_s, Z_{s+h}) = C(h) \\
\mathbb{E}[Z_s] = m
\end{gathered}
$$
Strict stationarity does not imply second-order stationarity as the first 2 moments may not be defined. And of course weak stationary does not imply strict stationarity. $C(h)$ is called the auto-correlation function. It is different from the covariance function. 

##### The intrinsic stationarity

A random function is said to have intrinsic stationarity if the following two conditions are met:

$$
\mathbb{E}[Z_{s+h} - Z_s] = 0 \\
V(Z_{s+h} - Z_s) = 2\gamma(h)
$$
The condition is no longer on the random function $Z(s)$ itself but rather on the increments $Z_{s+h} - Z_s$. The function $\gamma(h)$ is the semi-variogram function. 

The second-order stationarity implies the intrinsic one. The reverse is not true. 

#### Variogram

In the definition of intrinsic stationarity, the function $\gamma(h)$ was introduced. It is the semi-variogram. Out of the covariance function, the auto-correlation function and the variogram, the latter is the most often used as it relies on the least restrictive hypotheses of stationarity. 

It is a function that only depends on $h$, the distance between 2 points, and that describes the spatial dependence of a random field. It measures how much 2 locations separated by $h$ will vary in their $z$ value. Under the assumption of intrinsic stationarity on $Z$ the variogram $\gamma(h)$ is defined by: 

$$
\begin{equation}
\begin{split}
\gamma(h) & = V(Z_{s+h} - Z_{s}) \\
 & = \mathbb{E}\left[Z_{s+h} - Z_{s}\right]^2 - \mathbb{E}\left[\left(Z_{s+h} - Z_{s}\right)^2\right] \\
 & = \mathbb{E}\left[Z_{s+h} - Z_{s}\right]^2 - 0
\end{split}
\end{equation}
$$

Now that some basic concepts have been defined, let's dive into the kriging method. We will be defining ordinary Kriging.

## How it works {#how-it-works}

Recall that our goal is to find the weights of this equation $\widehat{Z_{s_0}} = \sum_{i = 1}^{n}\lambda_i Z_{s_i}$. We want to find the weights $\lambda_i$ required to predict the value $Z_{s_0}$. We want our estimate to be unbiased. In other words we want our predictions to be exact, on average. Mathematically we want: 

$$
\begin{equation}
\begin{split}
& \mathbb{E}[\widehat{z_{s_0}} - z_{s_0}] = 0 \\
\Leftrightarrow \ & \mathbb{E}\left[\sum_{i = 1}^{n}\lambda_i Z_{s_i} - Z_{s_0}\right] = 0 \\
\Leftrightarrow \ & \sum_{i = 1}^{n}\lambda_i \mathbb{E}[Z_{s_i}] - \mathbb{E}[Z_{s_0}] = 0 \\
\Leftrightarrow^* \ & \sum_{i = 1}^{n}\lambda_i m - m = 0 \\
\Leftrightarrow \ & \sum_{i = 1}^{n}\lambda_i = 1
\end{split}
\end{equation}
$$

To be able to go from line 3 to 4 we need the relation $\mathbb{E}[Z_{s_i}] = \mathbb{E}[Z_{s_0}] = m$. This is the second-order stationarity assumption that we suppose our random field $Z(s)$ fulfils. stationarity is more like a model assumption than an hypothesis to be tested. It is a way to simplify the model we wish to describe. 

The difference between 2 unbiased estimators is their variance. By minimising the variance of the error term, the model is ensured to be the _Best Linear Unbiased Estimator_. The weights $\lambda_1, \dots, \lambda_n$ are defined by the following optimisation problem.

$$
\begin{equation}
\displaystyle \min_{\lambda_1, \dots, \lambda_n} V\left(\widehat{Z_{s_0}} - Z_{s_0}\right)\\
\textrm{s.t.} \sum_{i=1}^{n} \lambda_i = 1\\
\end{equation}
$$

The [Lagrange Multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier) $\mu$ is used to solve the optimisation problem. The zeros of the Lagrangian function $\mathcal{L}$ are the optimisation problem solutions.

$$
\mathcal{L}(\lambda_1, \dots, \lambda_n, \mu) = V\left(\widehat{Z_{s_0}} - Z_{s_0}\right) - 2 \mu\left(\sum_{i=1}^{n} \lambda_i - 1\right)
$$
The weights that minimise the Lagrangian function are given by the following expression. The detailed explanation can be found in the [Annexe](#kriging-weights-calculus).

$$
\begin{bmatrix}
\lambda_1 \\ \vdots \\ \lambda_n \\ \mu
\end{bmatrix}
=
\begin{bmatrix}
\gamma(s_1-s_1) & \cdots & \gamma(s_1-s_n) &1 \\
\vdots & \ddots & \vdots  & \vdots \\
\gamma(s_n-s_1) & \cdots & \gamma(s_n-s_n) & 1 \\
1 &\cdots& 1 & 0 
\end{bmatrix}^{-1}
\begin{bmatrix}\gamma(s_1-s_0) \\ \vdots \\ \gamma(s_n-s_0) \\ 1\end{bmatrix}
$$

## Step-by-step code for Ordinary Kriging

In this section the code used to build an ordinary kriging model is given. The construction of the variogram needed to find the weights and the actual computation of the weights will be described. 

As for IDW the code will be detailed to explicitly show what is going on under the hood. This is for explanatory purpose only. The package [`geoR`](https://CRAN.R-project.org/package=geoR) contains higher-level functions to perform all the necessary steps of Kriging. The different higher-level functions that will be presented in this document comes from this package. The code for a complete Kriging model using functions from `geoR` will be presented as [the end of this section](#kriging-functions). 

### The semi-variogram

The semi-variogram is a key part of the Kriging model as it is used to compute the weights. It is then important to estimate it properly. First an empirical variogram is built, then a parametric one is fitted on it. 

A semi-variogram gives the strength of the association between 2 points based on the distance between them. The intuition is that when predicting the value at a new location, close points will have a stronger weight as their association strength is high. 

The semi-variogram is a function, $\gamma(h)$, that gives the semi-variance of 2 points separated by a distance $h$. The higher the value of the semi-variance, the lesser strong the association between the points. Our goal in this section is to build this function. First the empirical semi-variogram is built. The association strength is estimated as some distances, this forms a set of points on which a parametric model can be fitted. The parametric semi-variogram will be a function that takes one argument, the distance, and can be computed for every distances. 

#### The empirical semi-variogram

For a translation $h \in \mathbb{R}+$, $N_h = \left\{(s_i,s_j) : dist(s_i, s_j) = h; i,j = 1, \dots,n\right\}$ the set of points separated by distance $h$ and $|N_h|$ the number of points in this set, the empirical semi-variogram is defined by the following relation:

$$
\gamma(h) = \frac{1}{2|N_h|} \sum_{i=1}^{|N_h|}\left(Z_{s_{i+h}}-Z_{s_i}\right)^2
$$

Of course there won't be many pairs of points at exactly $h$ distance. So $N_h$ is actually defined as the set $\left\{(s_i,s_j) : dist(s_i, s_j) = h\pm \delta; i,j = 1, \dots,n\right\}$ with $\delta$ a tolerance range. In practice bins of distances are defined. 
To compute the empirical variogram we must start by calculating the value $\left(Z_{s_{i+h}}-Z_{s_i}\right)^2$ for every pair of points $(s_i, s_j), i,j = 1, \dots,n$.

```{r semi-variog-cloud, echo = TRUE}
vario_matrix <- outer(
  X = sample_points$z,
  Y = sample_points$z,
  FUN = function(x, y) (x - y)^2 / 2
)
```

```{r semi-variog-cloud-plot}
# Distance matrix between all points
dist_matrix <- dist(sample_points[, .(x, y)])

# Coerce matrix to long table so it is easier to plot later on (with the variogram)
dist_matrix <- dist_matrix %>%
  as.matrix() %>%
  as.data.table() %>%
  .[, point_a := 1:nrow(sample_points)] %>%
  melt(
    id.vars = "point_a",
    variable.name = "point_b",
    value.name = "distance"
  ) %>%
  .[, point_b := as.numeric(point_b)]

# Coerce to long table
vario_matrix <- vario_matrix %>%
  as.data.table() %>%
  setNames(as.character(1:nrow(sample_points))) %>%
  .[, point_a := 1:nrow(sample_points)] %>%
  melt(
    id.vars = "point_a",
    variable.name = "point_b",
    value.name = "semi_variance"
  ) %>%
  .[, point_b := as.numeric(point_b)]

# Build one table from distance and variogram
point_variogram <- merge(
  x = dist_matrix,
  y = vario_matrix,
  by = c("point_a", "point_b"),
  all = TRUE,
  sort = FALSE
)

# Visualisation of the variogram scatter plot
ggplot(
  data = point_variogram,
  mapping = aes(x = distance, y = semi_variance)
) +
  geom_point(
    alpha = .3
  ) +
  labs(
    x = "Distance",
    y = latex2exp::TeX("$\\frac{(Z_{s_i} - Z(s_j))^2}{2}$"),
    title = "Semi-variance by pair of points"
  )
```

The semi-variogram is computed using these points. Bins of distances are created and all the points inside each bin are averaged. The result is what we call the empirical semi-variogram. 
The first simple approach is to divide the distances into bins of equal width. This is simply obtained with the `cut()` function.

```{r semi-variog-cut1, echo = TRUE}
# Create classes of distance on which average semi variance will be computed
point_variogram[, group1 := cut(distance, 15)]
point_variogram[, x := define_center(group1)] # extract center of the class from 'cut' labels

# Compute the average semi-variance by distance group
empirical_variogram <- point_variogram[, .(count = .N, vario_mean = mean(semi_variance)), by = x]
```

The resulting points are visualised in the following figure. There is one point per group defined (15 in this case). The x-value of each point is the center of the distance bin, the y-value is the average of the $\left(Z_{s_{i+h}}-Z_{s_i}\right)^2$ value for each point in the bin. 

```{r semi-variog-cut1-plot}
plot_variogram(empirical_variogram)
```

There are several things one should pay attention to when analysing an empirical variogram: 

* the shape of the semi-variogram: recall the Tobler's first law of geography, distant things are less related than close ones. In the context of a variogram, it means that the semi-variance should be increasing with the distance. It is all the more true that when we will fit a parametric semi-variogram to the empirical one, the different models that we will use are all increasing with the distance. 
* the representativeness of each point: each point is the average of the value of all the points in the bin. If the number of points in the bin is too small or if the points have really different values then taking the point does not carry enough information.

Here the semi-variogram is not increasing with the distance, after 0.75. To evaluate the representativeness of each point we can plot the distribution of semi-variance by group using boxplots. 

```{r semi-variog-cut1-boxplot, fig.width=9}
plot_variogram_box(point_variogram, empirical_variogram)
```

We want the boxes to be as small as possible and to be all the same size. It is difficult to know when our choice of bin is good enough, there is not a single best choice.

Our second try at defining bins will consist in setting a max distance above which the semi-variance is not computed. This should solve the decreasing issue of our semi-variogram. We consider that 2 points separated by more than a specific value no longer don't influence each other and should not be taken into account when computing the semi-variogram. Besides the bins are constructed to be of equal size and not equal width. This is done using the function `ggplot2::cut_number()`. 

```{r semi-variog-cut2, echo = TRUE}
# Create evenly populated classes of distances
point_variogram[distance < .8, group3 := cut_number(distance, 10)]
point_variogram[, x := define_center(group3)]

# Compute the average semi-variance by distance group
empirical_variogram <- point_variogram[, .(count = .N, vario_mean = mean(semi_variance)), by = x]
```


```{r semi-variog-cut2-plot}
plot_variogram(empirical_variogram[!is.na(x)])
```

The semi-variogram is now increasing, on the distances of interest. Let's look at the distribution of each bins. 

```{r semi-variog-cut2-boxplot, fig.width=9}
plot_variogram_box(point_variogram[!is.na(x)], empirical_variogram[!is.na(x)])
```

By construction the bins all have the same number of points, the boxes are not too large. The most important part of a semi-variogram is its shape for low distances. Because usually only close points will have an impact on the prediction, it is important to have a good estimate of the association semi-variance at small distances. 

Our last try consists of bins that are smaller for small distances and then as before. 4 bins are defined for the first 15% of the data then 8 for the remaining 85%.

```{r semi-variog-cut3, echo = TRUE}
# Smaller groups at the beginning
point_variogram[distance < .8, group4 := cut(distance, breaks = quantile(distance, c(seq(0, .15, by = .05), seq(.2, 1, length.out = 9))))]
point_variogram[, x := define_center(group4)]

# Compute the average semi-variance by distance group
empirical_variogram <- point_variogram[, .(count = .N, vario_mean = mean(semi_variance)), by = x]
```


```{r semi-variog-cut3-plot}
plot_variogram(empirical_variogram[!is.na(x)])
```

The semi-variogram is now increasing, on the distances of interest. Let's look at the distribution of each bins. 

```{r semi-variog-cut3-boxplot, fig.width=9}
plot_variogram_box(point_variogram[!is.na(x)], empirical_variogram[!is.na(x)])
```

The four first bins contains less values than the other eight. The boxes are all quite small so this empirical semi-variogram is chosen as a basis for a parametric one. 

#### The parametric semi-variogram

Now that an empirical semi-variogram has been defined, it is possible to fit a parametric one on it. 

Parametric semi-variogam are pre-defined functions whose parameters are to be fitted on data. Different functions exist, 3 will be tested in this document, the gaussian one, the exponential one and the spheric one. 

```{r parametric-variog}
sigma <- 20
phi <- 100
nugget <- 0
x <- seq(0, 150, length.out = 1000)
vario_mod_spherical <- ifelse(
  test = x > phi,
  yes = nugget + sigma,
  no = nugget + sigma * (3 / 2 * (x / phi) - 1 / 2 * (x / phi)^3)
)
vario_mod_gaussian <- ifelse(
  test = x == 0,
  yes = 0,
  no = nugget + sigma * (1 - exp(-x^2 / (phi - 40)^2))
)
vario_mod_exponential <- ifelse(
  test = x == 0,
  yes = 0,
  no = nugget + sigma * (1 - exp(-x / (phi - 40)))
)
data.frame(
  x = rep(x, 3),
  y = c(vario_mod_spherical, vario_mod_gaussian, vario_mod_exponential),
  type = rep(c("Spherical", "Gaussian", "Exponential"), each = 1000)
) %>% 
  ggplot(
    mapping = aes(x = x, y = y, color = type)
  ) + 
  geom_line() + 
  labs(
    title = "Exemple of 3 parametric semi-variograms",
    x = "Distance",
    y = "Semi-variance",
    color = "Model"
  ) + 
  theme(
    legend.position = c(.8, .3)
  )
```

The functions are defined with 3 parameters:

* the sill $\sigma$ is the limit of the semi-variogram when $h \to \infty$.
* the range $\phi$ is the distance above which the semi-variance is close to the sill. By convention it is when 95% of the sill is obtained for semi-variogram with a sill (as some may tends towards $\infty$ when $h \to \infty$).
* the nugget $\gamma_0$ is the semi-variance when $h \to 0$. It represents the measurement error. Such parameter can be fitted like the other 2 but should probably be fixed by hand based on expertise on the process at stake. 

$$
\begin{gathered}
\text{Exponential : } \gamma(h) = \gamma_0 + \sigma^2\left[1 - \exp\left(-\frac{h}{\phi}\right)\right] \\
\text{Gaussian : } \gamma(h) = \gamma_0 + \sigma^2\left[1 - \exp\left(-\frac{h^2}{\phi^2}\right)\right] \\
\text{Spheric : } \gamma(h) = \begin{cases}
\gamma_0 + \sigma \ \text{ if } h > \phi\\
\gamma_0 + \sigma \left(\frac{3h}{2\phi} - \frac{1h^3}{2\phi^3}\right) \ \text{ else }
\end{cases}
\end{gathered}
$$
Other functions can be used but those are the most common ones. They convey a strong prior on the spatial dependance, there is a finite limit when $h \to \infty$. 

A numerical optimisation is used to fit the parameters. The function to minimise is the sum of squared errors between the parametric model values and the ones of the empirical semi-variogram. For instance, with $\gamma_1, \dots, \gamma_k$ the semi-variance of the $k$ bins $h_1, \dots, h_k$, the loss of the exponential model is:

$$
Loss(\gamma_0, \sigma, \phi) = \sum_{i=1}^{k}\left(\gamma_0 + \sigma^2\left[1 - \exp\left(-\frac{h_i}{\phi}\right)\right] - \gamma_i\right)^2
$$
We want the parameters $\gamma_0, \sigma, \phi$ that minimise the loss. Here we let $\gamma_0$ be adjusted as well. There are simply obtained with the `optim()` function. Here we specify a spherical model.

```{r variog_fit, echo = TRUE, eval = FALSE}
# Define the loss function needed for the optim function. It takes 3 arguments: the parameters
# to find, the distances and semi-variance of the empirical semi-variogram
loss_function <- function(params, x, vario_emp) {
  sigma <- params[1]
  phi <- params[2]
  nugget <- params[3]
  vario_mod <- ifelse(
      test = x > phi,
      yes = nugget + sigma,
      no = nugget + sigma * (3 / 2 * (x / phi) - 1 / 2 * (x / phi)^3)
    )
  loss <- sum((vario_emp - vario_mod)^2)
  return(loss)
}
spherical_param <- optim(
  par = c(2.2, .4, 0),
  fn = loss_function,
  x =  empirical_variogram$x[!is.na(empirical_variogram$x)],
  vario_emp = empirical_variogram$vario_mean[!is.na(empirical_variogram$x)]
)
```

The obtained parameters, the object `spherical_param`, can then be used to build the semi-variogram function.

```{r variog_func, echo = TRUE, eval = FALSE}
vario_func <- function(x) {
  sigma <- spherical_param$par[1]
  phi <- spherical_param$par[2]
  nugget <- spherical_param$par[3]
  return(ifelse(
      test = x > phi,
      yes = nugget + sigma,
      no = nugget + sigma * (3 / 2 * (x / phi) - 1 / 2 * (x / phi)^3)
    ))
}
```

The quality of adjustment of the 3 parametric semi-variograms to the empirical one can be assess visually and by looking at the loss value. 

```{r variog_fit_2}
loss_function <- function(params, x, vario_emp, cov_model) {
  sigma <- params[1]
  phi <- params[2]
  nugget <- params[3]
  if (cov_model == "spherical") {
    vario_mod <- ifelse(
      test = x > phi,
      yes = nugget + sigma,
      no = nugget + sigma * (3 / 2 * (x / phi) - 1 / 2 * (x / phi)^3)
    )
  } else if (cov_model == "gaussian") {
    vario_mod <- ifelse(
      test = x == 0,
      yes = 0,
      no = nugget + sigma * (1 - exp(-x^2 / phi^2))
    )
  } else if (cov_model == "exponential") {
    vario_mod <- ifelse(
      test = x == 0,
      yes = 0,
      no = nugget + sigma * (1 - exp(-x / phi))
    )
  }
  loss <- sum((vario_emp - vario_mod)^2)
  return(loss)
}

# Numeric optimisation to find the best parameters
gausian_param <- optim(
  par = c(2.2, .4, 0),
  fn = loss_function,
  x =  empirical_variogram$x[!is.na(empirical_variogram$x)],
  vario_emp = empirical_variogram$vario_mean[!is.na(empirical_variogram$x)],
  cov_model = "gaussian"
)
spherical_param <- optim(
  par = c(2.2, .4, 0),
  fn = loss_function,
  x =  empirical_variogram$x[!is.na(empirical_variogram$x)],
  vario_emp = empirical_variogram$vario_mean[!is.na(empirical_variogram$x)],
  cov_model = "spherical"
)
exponential_param <- optim(
  par = c(2.2, .4, 0),
  fn = loss_function,
  x =  empirical_variogram$x[!is.na(empirical_variogram$x)],
  vario_emp = empirical_variogram$vario_mean[!is.na(empirical_variogram$x)],
  cov_model = "exponential"
)
# Visualise the parametric variogram and the empirical one
vario_fit_data <- data.frame(
  x = rep(seq(0, .8, length.out = 1000), 3),
  type = rep(c("Exponential", "Gaussian", "Spherical"), each = 1000),
  y = c(
    exponential_param$par[3] + exponential_param$par[1] * (1 - exp(-seq(0, .8, length.out = 1000) / exponential_param$par[2])),
    gausian_param$par[3] + gausian_param$par[1] * (1 - exp(-seq(0, .8, length.out = 1000)^2 / gausian_param$par[2]^2)),
    ifelse(
      test = seq(0, .8, length.out = 1000) > spherical_param$par[2],
      yes = spherical_param$par[3] + spherical_param$par[1],
      no = spherical_param$par[3] + spherical_param$par[1] * (3 / 2 * (seq(0, .8, length.out = 1000) / spherical_param$par[2]) - 1 / 2 * (seq(0, .8, length.out = 1000) / spherical_param$par[2])^3)
    )
  )
)
plot_variogram(empirical_variogram[!is.na(x)]) +
  geom_line(
    data = vario_fit_data,
    mapping = aes(x = x, y = y, color = type)
  ) +
  guides(
    colour = guide_legend(override.aes = list(size = 2))
  ) +
  scale_colour_discrete(
    labels = c(
      "Exponential" = paste0("Exponential: ", round(exponential_param$value, 2)),
      "Gaussian" = paste0("Gaussian: ", round(gausian_param$value, 2)),
      "Spherical" = paste0("Spherical: ", round(spherical_param$value, 2))
    )
  ) + 
  labs(
    title = "3 parametric semi-variograms fitted on the empirical one",
    subtitle = "The loss value of each parametric model is given in the legend.",
    y = "Semi-variance",
    color = "Parametric model"
  ) +
  theme(
    legend.position = c(.8, .2)
  )
```

The exponential model has the lowest loss value. Nevertheless we want our semi-variogram to be precise on low distances and the gaussian and spherical ones seem to be better adjusted at lower distances. The exponential model has a better overall fit because it is closer to the last points on average. The spherical model is the one that will be selected for now on. 

### The Kriging weights

The Kriging weights, defined in the _how it works_ [section](#how-it-works), are computed for a new point $s_0$ with the following expression. $\mu$ is the Lagrange multiplier, coming from the optimisation resolution process.

$$
\begin{bmatrix}
\omega_1 \\ \vdots \\ \omega_n \\ \mu
\end{bmatrix}
=
\begin{bmatrix}
\gamma(s_1,s_1) & \cdots & \gamma(s_1,s_n) &1 \\
\vdots & \ddots & \vdots  & \vdots \\
\gamma(s_n,s_1) & \cdots & \gamma(s_n,s_n) & 1 \\
1 &\cdots& 1 & 0 
\end{bmatrix}^{-1}
\begin{bmatrix}\gamma(s_1,s_0) \\ \vdots \\ \gamma(s_n,s_0) \\ 1\end{bmatrix}
$$
Now that the function $\gamma$ is defined, computing the weights $\omega_1, \dots, \omega_n$ is simply a matter of matrix inversion and multiplication. 

The following piece of code does the weight computation for a single point. It starts by creating the needed matrix and vector and then performs the multiplication to obtain the weights.

```{r kriging-weights, echo = TRUE, eval = FALSE}
# Coordinates of the point s0 to predict
point_pred <- c(.5, .5)

# # Add row names to easily spot points
# rownames(sample_points) <- paste0("s", 1:nrow(sample_points))

# Distance matrix of s1,...sn and s0
matrix_dist <- as.matrix(sample_points[, c("x", "y")]) %>% # transform the coordinates into a matrix
  rbind("s_pred" = point_pred) %>% # add s0
  wordspace::dist.matrix(method = "euclidean") # compute the distance between all points

# Extract the distance between s0 and sample points s1,...,sn (distances needed to compute the vector)
predict_dist <- matrix_dist[1:100, 101]

# Remove distance with s0 (distances needed to compute the matrix)
sample_dist <- matrix_dist[1:100, 1:100]

# Function to compute the variogram
vario_func <- function(x) {
  return(ifelse(
    test = x > spherical_param$par[2],
    yes = spherical_param$par[3] + spherical_param$par[1],
    no = spherical_param$par[3] + spherical_param$par[1] * (3 / 2 * (x / spherical_param$par[2]) - 1 / 2 * (x / spherical_param$par[2])^3)
  ))
}

# Variogram matrix
variogram_matrix <- apply(
  X = sample_dist,
  MARGIN = c(1, 2),
  FUN = function(x) vario_func(x)
)

# Add the extra row and column of 1 (and 0 on last cell) to complete the matrix
variogram_matrix <- rbind(variogram_matrix, 1)
variogram_matrix <- cbind(variogram_matrix, 1)
variogram_matrix[nrow(variogram_matrix), ncol(variogram_matrix)] <- 0

# Inverse the matrix
variogram_matrix_inv <- solve(variogram_matrix)

# Variogram vector
variogram_vec <- vario_func(predict_dist)

# Add extra 1
variogram_vec <- c(variogram_vec, 1)

# Compute the vector of weights
weights <- variogram_matrix_inv %*% variogram_vec

# Compute the value at s0 by multiplying the weights and the values at sample points
sum(weights[1:100] * sample_points$z)
```

The prediction for every point of the grid is simply obtained by applying the above code for every point s0. Something like the following piece of code could be used. It is super slow and of course a better alternative will be presented.

```{r kriging-weights-all, echo = TRUE, eval = FALSE}
pb <- progress_bar$new(total = nrow(simulated_grid), format = "[:bar] :current/:total (:percent) :elapsedfull")
grid_pred <- sapply(
  X = seq_len(nrow(simulated_grid)),
  FUN = function(i) {
    point_pred <- as.numeric(simulated_grid[i, c("x", "y")])
    matrix_dist <- as.matrix(sample_points[, c("x", "y")]) %>%
      rbind("s_pred" = point_pred) %>%
      wordspace::dist.matrix(method = "euclidean")
    predict_dist <- matrix_dist[1:100, 101]
    sample_dist <- matrix_dist[1:100, 1:100]
    vario_func <- function(x) {
      return(ifelse(
        test = x > spherical_param$par[2],
        yes = spherical_param$par[3] + spherical_param$par[1],
        no = spherical_param$par[3] + spherical_param$par[1] * (3 / 2 * (x / spherical_param$par[2]) - 1 / 2 * (x / spherical_param$par[2])^3)
      ))
    }
    variogram_matrix <- apply(
      X = sample_dist,
      MARGIN = c(1, 2),
      FUN = function(x) vario_func(x)
    )
    variogram_matrix <- rbind(variogram_matrix, 1)
    variogram_matrix <- cbind(variogram_matrix, 1)
    variogram_matrix[nrow(variogram_matrix), ncol(variogram_matrix)] <- 0
    variogram_matrix_inv <- solve(variogram_matrix)
    variogram_vec <- vario_func(predict_dist)
    variogram_vec <- c(variogram_vec, 1)
    weights <- variogram_matrix_inv %*% variogram_vec
    pb$tick()
    return(sum(weights[1:100] * sample_points$z))
  }
)
```


```{r kriging-weights-map}
grid_pred <- readRDS("../output/kriging_pred.rds")

data.table(simulated_grid[, c("x", "y")], "pred" = grid_pred, "true" = simulated_grid$z) %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Prediction with Kriging and real data") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("pred" = "Kriging", "true" = "Real data"))
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

The resulting map is quite like the true data but in a smoother way. Kriging produces smooth prediction (it is a a weighted average). 

## The geoR package {#kriging-functions}

Of course in practice there is no need to write all the above code. 

### Defining the semi-variogram with high-level functions

```{r variog-code, echo = TRUE, eval = FALSE}
# Create a geodata object needed for all functions in the geoR package
geodata <- as.geodata(sample_points)

# Visualise the variogram cloud (semi-variance by pair of points)
variogram_c <- variog(geodata, option = "cloud")
plot(variogram_c)

# Build the empirical semi-variogram with default argument (13 bins, no maximal distance)
variogram <- variog(geodata)
plot(variogram)

# Visualise the distribution of semi-variance by bin
variogram_c <- variog(geodata, bin.cloud = TRUE)
plot(variogram_c)

# Build the empirical semi-variogram with default argument (13 bins, no maximal distance) 
variogram <- variog(
  geodata = geodata,
  max.dist = 70, # Specify the maximum distance at which the semi-variance is computed
  breaks = quantile(point_variogram$distance, c(seq(0, .15, by = .05), seq(.2, 1, length.out = 9))) # limits of the bins to create
)
plot(variogram)

# Fit an exponential parametric model on the empirical semi-variogram
vario_fit_spherical <- variofit(
  vario = variogram, # the empirical variogram
  cov.model = "spherical", # type of parametric model to adjust
  fix.nugget = FALSE # should the nugget be a fixed parameter set with the argument nugget?
)
```

### Computing the Kriging weights

```{r krigin-code, echo = TRUE, eval = FALSE}
# Define the point where the predictions should be done
grid <- expand.grid(x = seq(0, 1, length.out = 101), y = seq(0, 1, length.out = 101))

# Build the Kriging model with the spherical variogram
krige_control <- krige.control(
  type.krige = "ok", # ordinary Kriging
  obj.model = vario_fit_spherical
)
krige <- krige.conv(
  geodata = geodata, # observations
  locations = grid, # locations of the points to predict
  krige = krige_control
)
```

Let's check that indeed both solution produce the same output. The test is performed with 

```{r kriging-check}
geodata <- as.geodata(sample_points)
variogram <- variog(
  geodata = geodata,
  max.dist = .8,
  breaks = levels(point_variogram$group4) %>%
      as.character() %>%
      stri_replace_all_regex("(\\(|\\[|\\])", "") %>%
      stri_split_fixed(",") %>% 
    sapply(as.numeric) %>% 
    as.vector() %>% 
    unique()
)
vario_fit_spherical <- variofit(
  vario = variogram, # the empirical variogram
  cov.model = "spherical", # type of parametric model to adjust
  fix.nugget = TRUE,
  nugget = 0
)
vario_fit_spherical$cov.pars <- spherical_param$par[1:2]
grid <- expand.grid(x = seq(0, 1, length.out = 101), y = seq(0, 1, length.out = 101))
krige_control <- krige.control(
  type.krige = "ok", # ordinary Kriging
  obj.model = vario_fit_spherical
)
krige <- krige.conv(
  geodata = geodata, # observations
  locations = simulated_grid[, c("x", "y")], # locations of the points to predict
  krige = krige_control
)
data.table(simulated_grid[, c("x", "y")], "pred" = grid_pred, "pred_geor" = krige$predict) %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Prediction with Kriging and real data") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("pred" = "Low-level functions", "pred_geor" = "geoR package"))
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

# Annexes

## Kriging weights {#kriging-weights-calculus}

The weights are the solution to the following optimisation problem.

$$
\begin{equation}
\displaystyle \min_{\lambda_1, \dots, \lambda_n} V\left(\widehat{Z_{s_0}} - Z_{s_0}\right) \\
\textrm{s.t.} \sum_{i=1}^{n} \lambda_i = 1 \\
\end{equation}
$$

The [Lagrange Multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier) $\mu$ is used to solve the optimisation problem. The zeros of the Lagrangian function $\mathcal{L}$ are the optimisation problem solutions.

$$
\mathcal{L}(\lambda_1, \dots, \lambda_n, \mu) = V\left(\widehat{Z_{s_0}} - Z_{s_0}\right) - 2 \mu\left(\sum_{i=1}^{n} \lambda_i - 1\right)
$$

The objective is to express the Lagrangian function in terms of the variogram. Let's decompose $V\left(\widehat{Z_{s_0}} - Z_{s_0}\right)$. 

$$
\begin{equation}
\begin{split}
V\left(\widehat{Z_{s_0}} - Z_{s_0}\right) & = \mathbb{E}\left[\left(\widehat{Z_{s_0}} - Z_{s_0} \right)^2\right] -  \mathbb{E}\left[\widehat{Z_{s_0}} - Z_{s_0}\right]^2 \\
 & = \mathbb{E}\left[\left(\widehat{Z_{s_0}} - Z_{s_0} \right)^2\right] -  0 \\
  & = \mathbb{E}\left[\left(\sum_{i=1}^{n}\lambda_iZ_{s_i} - Z_{s_0} \right)^2\right]
\end{split}
\end{equation}
$$

And the term $\left(\sum_{i=1}^{n}\lambda_iZ_{s_i} - Z_{s_0} \right)^2$ can be expressed in the following way.

$$
\begin{equation}
\begin{split}
\left(\sum_{i=1}^{n}\lambda_iZ_{s_i} - Z_{s_0} \right)^2 & = \left(\sum_{i=1}^{n}\lambda_iZ_{s_i}\right)^2 - 2Z_{s_0}\sum_{i=1}^{n}\lambda_iZ_{s_i} + Z_{s_0}^2 \\
 & = \sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_jZ_{s_i}Z_{s_j} - 2Z_{s_0}\sum_{i=1}^{n}\lambda_iZ_{s_i} + Z_{s_0}^2 \\
 & = \left(\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_jZ_{s_i}Z_{s_j} - \sum_{i=1}^{n}\lambda_iZ_{s_i}^2\right) - \left(\sum_{i=1}^{n}\lambda_iZ_{s_i}^2 - 2Z_{s_0}\sum_{i=1}^{n}\lambda_iZ_{s_i} + Z_{s_0}^2\right) \\
 & = A - B 
\end{split}
\end{equation}
$$

$$
\begin{equation}
\begin{split}
A & = \sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_jZ_{s_i}Z_{s_j} - \sum_{i=1}^{n}\lambda_iZ_{s_i}^2 \\
 & = \sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_jZ_{s_i}Z_{s_j} - \sum_{j=1}^{n}\lambda_j\sum_{i=1}^{n}\lambda_iZ_{s_i}^2 \\
 & = \sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_jZ_{s_i}Z_{s_j} - \frac{1}{2}\sum_{j=1}^{n}\sum_{i=1}^{n}\lambda_j\lambda_iZ_{s_i}^2 - \frac{1}{2}\sum_{j=1}^{n}\sum_{i=1}^{n}\lambda_j\lambda_iZ_{s_j}^2 \\
 & = -\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j\left(-2Z_{s_i}Z_{s_j} + Z_{s_i}^2 + Z_{s_j}^2\right) \\
 & = -\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j\left(Z_{s_i} - Z_{s_j}\right)^2 \\
\end{split}
\end{equation}
$$

$$
\begin{equation}
\begin{split}
B & = \sum_{i=1}^{n}\lambda_iZ_{s_i}^2 - 2Z_{s_0}\sum_{i=1}^{n}\lambda_iZ_{s_i} + Z_{s_0}^2 \\
 & = \sum_{i=1}^{n}\lambda_iZ_{s_i}^2 - 2Z_{s_0}\sum_{i=1}^{n}\lambda_iZ_{s_i} + \sum_{i=1}^{n}\lambda_iZ_{s_0}^2 \\
 & = \sum_{i=1}^{n}\lambda_i\left(Z_{s_i}^2 - 2 Z_{s_0}Z_{s_i} + Z_{s_0}^2\right) \\
 & = \sum_{i=1}^{n}\lambda_i\left(Z_{s_0} - Z_{s_i}\right)^2
\end{split}
\end{equation}
$$

Back to the expression of $V\left(\widehat{Z_{s_0}} - Z_{s_0}\right)$. 

$$
\begin{equation}
\begin{split}
V\left(\widehat{Z_{s_0}} - Z_{s_0}\right) & = \mathbb{E}\left[-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j\left(Z_{s_i} - Z_{s_j}\right)^2\right] + \mathbb{E}\left[\sum_{i=1}^{n}\lambda_i\left(Z_{s_0} - Z_{s_i}\right)^2\right] \\
 & = -\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j\frac{1}{2}\mathbb{E}\left[\left(Z_{s_i} - Z_{s_j}\right)^2\right] + 2\sum_{i=1}^{n}\lambda_i\frac{1}{2}\mathbb{E}\left[\left(Z_{s_0} - Z_{s_i}\right)^2\right] \\
 & = -\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j\gamma(s_i-s_j) + 2\sum_{i=1}^{n}\lambda_i\gamma(s_0-s_i)
\end{split}
\end{equation}
$$

Adding the Lagrange multiplier we have:

$$
\mathcal{L}(\lambda_1, \dots, \lambda_n, \mu) = -\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j\gamma(s_i-s_j) + 2\sum_{i=1}^{n}\lambda_i\gamma(s_0-s_i) - 2 \mu\left(\sum_{i=1}^{n} \lambda_i - 1\right)
$$

To find the weights that minimise the variance the Lagrangian function should be derived and the zeros found. 

$$
\begin{equation}
\begin{split}
\begin{cases}
\frac{\partial \mathcal{L}}{\partial \lambda_1} = 0 \\
\dots \\
\frac{\partial \mathcal{L}}{\partial \lambda_n} = 0 \\
\frac{\partial \mathcal{L}}{\partial \mu} = 0
\end{cases} & \Leftrightarrow & 
\begin{cases}
-2\sum_{i=1}^{n}\lambda_i\gamma(s_i-s_1) + 2 \gamma(s_0 - s_1) - 2 \mu = 0\\
\dots \\
-2\sum_{i=1}^{n}\lambda_i\gamma(s_i-s_n) + 2 \gamma(s_0 - s_n) - 2 \mu = 0 \\
\sum_{i=1}^{n}\lambda_i - 1 = 0
\end{cases}
\\
 & \Leftrightarrow & 
 \begin{cases}
\sum_{i=1}^{n}\lambda_i\gamma(s_i-s_1) + \mu = \gamma(s_0 - s_1)\\
\dots \\
\sum_{i=1}^{n}\lambda_i\gamma(s_i-s_n) + \mu =  \gamma(s_0 - s_n)  \\
\sum_{i=1}^{n}\lambda_i = 1
\end{cases} 
\\
  & \Leftrightarrow & 
  \begin{bmatrix}
   \sum_{i=1}^{n}\lambda_i\gamma(s_i-s_1) + 1 \mu \\
   \dots \\
   \sum_{i=1}^{n}\lambda_i\gamma(s_i-s_n) + 1 \mu \\
   \sum_{i=1}^{n}\lambda_i
   \end{bmatrix} 
   = 
   \begin{bmatrix}
   \gamma(s_0-s_1) \\
   \dots \\
   \gamma(s_0-s_n) \\
   1
   \end{bmatrix}
\\
  & \Leftrightarrow & 
  \begin{bmatrix}
   \lambda_1 \\
   \dots \\
   \lambda_n \\
   \mu
   \end{bmatrix} ^\top
   \begin{bmatrix}
   \gamma(s_1-s_1) & \dots & \gamma(s_1-s_n) & 1 \\
   \vdots & \ddots & \vdots & \vdots  \\
   \gamma(s_n-s_1) & \dots & \gamma(s_n-s_n) & 1 \\
   1 & \dots & 1 & 0
   \end{bmatrix} 
   = 
   \begin{bmatrix}
   \gamma(s_0-s_1) \\
   \dots \\
   \gamma(s_0-s_n) \\
   1
   \end{bmatrix}
\end{split}
\end{equation}
$$

Finally we have an expression for the weights.

$$
\begin{bmatrix}
\lambda_1 \\ \vdots \\ \lambda_n \\ \mu
\end{bmatrix}
=
\begin{bmatrix}
\gamma(s_1-s_1) & \cdots & \gamma(s_1-s_n) &1 \\
\vdots & \ddots & \vdots  & \vdots \\
\gamma(s_n-s_1) & \cdots & \gamma(s_n-s_n) & 1 \\
1 &\cdots& 1 & 0 
\end{bmatrix}^{-1}
\begin{bmatrix}\gamma(s_1-s_0) \\ \vdots \\ \gamma(s_n-s_0) \\ 1\end{bmatrix}
$$

# References