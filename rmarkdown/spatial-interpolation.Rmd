---
title: "Spatial interpolation"
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    theme: cerulean
    highlight: tango
    css: style.css
bibliography: biblio.bib
biblio-style: "apalike"
link-citations: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "all",
      } 
  }
});
</script>

<!-- https://groups.google.com/forum/#!topic/knitr/OMvT03PtGPM --> 

```{r initial_chunk, echo = FALSE, warning = FALSE, message = FALSE}
library("knitr")
opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, vcache = FALSE, fig.align = 'center', dpi = 300, out.width = '75%')
```

```{r set-up}
library("cowplot") # Export plots
library("data.table") # Fast dataset manipulation
import::from("fields", "rdist") # rdist function
library("ggplot2") # Data visualisations using the Grammar of Graphics
library("ggrepel") # Text lables
library("gstat") # Geostatistical modelling
library("magrittr") # Pipe operators
source("../src/helpers.R") # Load custom functions

# Set default ggplot theme
theme_set(
  theme_light(
  base_size = 12
  ) +
  theme(
    text = element_text(family = "Gibson", colour = "gray10"),
    panel.border = element_blank(),
    axis.line = element_line(colour = "gray50", size = .5),
    # axis.ticks = element_blank(),
    strip.background = element_rect(colour = "gray50", fill = "transparent", size = .7),
    strip.text.x = element_text(colour = "gray10"),
    strip.text.y = element_text(colour = "gray10"),
    legend.key.size = unit(.7, "cm")
  )
)

# Set default scales
scale_colour_continuous <- function(...) ggplot2::scale_colour_viridis_c(..., option = "viridis")
scale_colour_discrete <- function(...) ggplot2::scale_colour_viridis_d(..., option = "viridis")
scale_fill_continuous <- function(...) ggplot2::scale_fill_viridis_c(..., option = "viridis")
scale_fill_discrete <- function(...) ggplot2::scale_fill_viridis_d(..., option = "viridis")
```


TODO : Introduction

#### The mathematical problem

Let's say you have $n$ measures $z_1, \dots, z_n$ of some variable $Z$ at $n$ points in some region of space $R \subset \in \mathbb{R}^2$. You want to know the value of $Z$ at every points in that region $R$. You have to define a function that for each point $(x, y) \in R$ associates a value $z \in \mathbb{R}$.

$$
(x, y) \in \mathbb{R}^2 \rightarrow z \in \mathbb{R}
$$

The only input data as your disposal are $n$ estimations of $f$ at sample points $s_1, \dots , s_n$, $s_i \in \mathbb{R}^2, i = 1 \dots n$. 

Different methods exists [see @overview] and most have the same general formula. The prediction $\widehat{z_0}$ for a point $s_0 \in \mathbb{R}^2$ is given by the following relation:

$$
\widehat{z_0} = \sum_{i = 1}^{n}\lambda_i z_i
$$

The way the weights $\lambda_1, \dots , \lambda_n$ is different for each methods. 4 methods are presented in this document: 

* Inverse Distance Weighting (IDW), 
* Triangular Irregular Networks (TIN), 
* Gaussian Process (GP)
* Kriging

#### A simple map as a case study

To illustrate the different methods we will use a simple use case, a simulated normal variable, $Z$, on a $[0, 100] \times [0, 100]$ grid. A random sample of 100 points are selected and will be used as the basis for the spatial interpolation. 

```{r grid, out.width="100%"}
simulated_grid <- readRDS("../data/grf_spheric_01x01.rds")
set.seed(123)
sample_points <- simulated_grid[sample(1:10201, 100), ]
plot_map(simulated_grid, "The 100 sample points on the true map") +
  geom_point(
    data = sample_points,
    mapping = aes(x = x, y = y),
    size = 1
  )
```

Having true data will make it possible to compare the solution to the real data and to evaluate its performance. Of course in a real life scenario you do not have the real data, hence for each method, a more challenging use case will also be presented. 

# Inverse Distance Weighting

## How it works

This methods uses the previous equation to predict new values. The weights $\lambda_1, \dots , \lambda_n$ are defined by taking the inverse of the distance between the point to predict, $s_0$, and the sample points, at a pecific power $p$. This method directly considers that the closer a point is to another the more alike they are. For $s_0$, $\forall i = 1\dots n$ and $p \in \mathbb{N}$,

$$
\lambda_i = \frac{\left(dist(s_0, s_j)^p\right)^{-1}}{\left(\sum_{j=1}^{n}dist(s_0, s_j)^p\right)^{-1}}
$$

The distance used is usually the euclidean distance $dist(a, b) = ||a-b||^2$. The parameter $p$ define the rate at which the influence of a point to another decreases. The higher $p$ the fastest the decrease of influence.

```{r power}
x_seq <- seq(0, 10, length.out = 11)
data <- data.table(
    x = rep(x_seq, 5),
    y = c(1 / x_seq^0, 1 / x_seq^1, 1 / x_seq^2, 1 / x_seq^3, 1 / x_seq^4),
    color = rep(as.character(0:4), each = 11)
  ) %>% 
  .[x == 3, label := paste("p =", color), by = color]
ggplot(
  data = data, 
  mapping = aes(x = x, y = y, color = color, group = color)
) + 
  geom_line(
    show.legend = FALSE
  ) + 
  scale_x_continuous(
    breaks = seq(2, 10, length.out = 5)
  ) + 
  geom_label_repel(
    mapping = aes(label = label),
    nudge_x = 1,
    show.legend = FALSE
  ) + 
  labs(
    x = "Distance",
    y = "Weight",
    title = "Comparison of different power orders"
  )
```

## The simple case study

First let's apply IDW on our simple sample points. Recall we have 100 points and we want to predict the value of the variable of interest $Z$ for every point of the grid $[0, 100] \times [0, 100]$.

Building the interpolated map means making a prediction for each point in the grid. What we have to do is then to compute the distance matrix between each point and the sample points, use these distances to build the weights and build our prediction according to equation 2 $\widehat{z_0} = \sum_{i = 1}^{n}\lambda_i z_i$. 

### Step by step code {#idw-step-by-step}

The process to make predictions on the grid is presented in the following code. Of course specific functions exist to do just that but let's break it piece by piece first. 

The order used here is 5, we will see later on how to define this value.

```{r idw_base, echo = TRUE}
# First compute the distance matrix between all points of the grid and all sample points
distance <- rdist(simulated_grid[, c("x", "y")], sample_points[, c("x", "y")])

# Define the power order
p <- 5

# Take the inverse of every element
weights <- apply(
  X = distance,
  MARGIN = 1:2,
  FUN = function(x) 1 / x^p
)

# Divide (FUN = "/") the weights by the row sum (STATS = rowSums(weights)) to every element of the
# weights matrix (x = weights) row wise (MARGIN = 1)
weights <- sweep(
  x = weights,
  MARGIN = 1,
  STATS = rowSums(weights),
  FUN = "/"
)

# Multiply the weights matrix with the z value of the sample points
pred <- weights %*% sample_points$z
```

We can now visualize our final prediction and compare it to the real data.

```{r idw_pred_map}
type_lables <- c("IDW map", "Real map")
names(type_lables) <- c("pred", "true")
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "true" = simulated_grid$z) %>%
  merge(
    y = sample_points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
plot_map("Prediction with IDW and real data") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = type_lables)
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

The map is globally the same but much more smooth. It displays some "bubbles" around the sample points which is of course coherent with the fact that each point has a strong influence only on its immediate neighborhood.

### Choosing the order $p$

The order $p$ was set to 5 in the previous model. Why not 3 or 6 or even 10? The choice of $p$ depends on the data you have and one should always try to find the most appropriate value. One way to do so is to evaluate the error of prediction using a leave-one-out estimation. For each point in the sample a prediction is made using all the other points. The estimation of the error is then the average error observed. 

More specifically if we have for $n$ points $s_1, \dots, s_n$, the leave-one-out error estimation is computed as follows:

$$
E_{LOO} = \sqrt{\frac{1}{n} \sum_{i=1}^{n}(z_i - \sum_{j=1, j \ne i}^{n}\lambda_j z_j)^2}
$$

The weights are computed as defined in equation 3. This error is computed for different values of $p$ and the value of $p$ that minimizes the error is to be chosen. 

The following piece of code illustrates this method. The error is estimated for $p = 1\dots 20$.

```{r loo, echo = TRUE}
rmse_errors <- sapply(
  X = 1:20, # Values of p
  FUN = function(p) { # Compute the RMSE estimation for each value of p
    pred_sample <- numeric(length = 100)
    for (i in length(pred_sample)) {
      # Predict point i and remove it from sample points
      distance <- rdist(sample_points[i, c("x", "y")], sample_points[-i, c("x", "y")])
      weights <- apply(
        X = distance,
        MARGIN = 1:2,
        FUN = function(x) 1 / x^p
      )
      weights <- sweep(
        x = weights,
        MARGIN = 1,
        STATS = rowSums(weights),
        FUN = "/"
      )
      pred_sample[i] <- weights %*% sample_points[-i, ]$z %>% as.numeric()
    }
    return(sqrt(sum((pred_sample - sample_points$z)^2) / nrow(sample_points)))
  }
)
```


```{r loo_plot}
ggplot(
  data = data.frame(x = 1:20, y = rmse_errors),
  mapping = aes(x = x, y = y)
) +
  geom_line() +
  labs(
    x = "Power order",
    y = "Leave-one-out RMSE",
    title = "RMSE for different orders p"
  )
```


The RMSE (estimated with leave-one-out) first decreases a lot and reach a plateau around 5. The actual best value is `r which.min(rmse_errors)` but from this plot one can see that choosing 5 or 20 does not make a huge difference. One reason coulb be that 5 is high enough to merely give all the weight to the closest point of the point to predict. The map obtained with $p=5$ looks a bit like a map we could have obtained with a 1-nearest-neighbour model.


### The `idw` function

The package `gstat` has an `idw()` function that does exactly what we did. It takes 5 arguments: 

* `formula` a formula giving the name of the target variable. For IDW the right hand side is 1, meaning no independant variables.
* `locations` a formula giving the names of the coordinates variables.
* `data` the name of the dataset containing the variables defined in `formula` and `locations`.
* `newdata` a table with the coordinates where to make predictions.
* `idp` the power order to use.

```{r idw, echo = TRUE}
pred_gstat <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = sample_points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5
)
```

As a sanity check let's compare the map obtained from the code of the [previous section](#idw-step-by-step). 

```{r idw_sanity}
names(pred_gstat) <- c("x", "y", "z", "var")
data.table(simulated_grid[, c("x", "y")], "idw_base" = pred[, 1], "idw_gstat" = pred_gstat$z) %>%
  merge(
    y = sample_points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(idw_base), idw_base := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Prediction with IDW and gstat IDW") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("idw_base" = "Step by step", "idw_gstat" = "IDW function"))) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```


# Help

$$
\begin{equation} 
\begin{split}
\mathrm{Var}(\hat{\beta}) & =\mathrm{Var}((X'X)^{-1}X'y)\\
 & =(X'X)^{-1}X'\mathrm{Var}(y)((X'X)^{-1}X')'\\
 & =(X'X)^{-1}X'\mathrm{Var}(y)X(X'X)^{-1}\\
 & =(X'X)^{-1}X'\sigma^{2}IX(X'X)^{-1}\\
 & =(X'X)^{-1}\sigma^{2}
\end{split}
\end{equation} 
$$

# References