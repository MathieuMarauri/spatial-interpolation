---
title: "Spatial interpolation in R"
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    theme: cerulean
    highlight: tango
    css: style.css
bibliography: biblio.bib
biblio-style: "apalike"
link-citations: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "all",
      } 
  }
});
</script>

<!-- https://groups.google.com/forum/#!topic/knitr/OMvT03PtGPM --> 

```{r initial_chunk, echo = FALSE, warning = FALSE, message = FALSE}
library("knitr")
opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, vcache = FALSE, fig.align = 'center', dpi = 300, out.width = '75%')
```

```{r set-up}
library("cowplot") # Export plots
library("data.table") # Fast dataset manipulation
import::from("fields", "rdist") # rdist function
library("ggplot2") # Data visualisations using the Grammar of Graphics
library("ggrepel") # Text lables
library("gstat") # Geostatistical modelling
library("magrittr") # Pipe operators
source("../src/helpers.R") # Load custom functions

# Set default ggplot theme
theme_set(
  theme_light(
  base_size = 12
  ) +
  theme(
    text = element_text(family = "Gibson", colour = "gray10"),
    panel.border = element_blank(),
    axis.line = element_line(colour = "gray50", size = .5),
    # axis.ticks = element_blank(),
    strip.background = element_rect(colour = "gray50", fill = "transparent", size = .7),
    strip.text.x = element_text(colour = "gray10"),
    strip.text.y = element_text(colour = "gray10"),
    legend.key.size = unit(.7, "cm")
  )
)

# Set default scales
scale_colour_continuous <- function(...) ggplot2::scale_colour_viridis_c(..., option = "viridis")
scale_colour_discrete <- function(...) ggplot2::scale_colour_viridis_d(..., option = "viridis")
scale_fill_continuous <- function(...) ggplot2::scale_fill_viridis_c(..., option = "viridis")
scale_fill_discrete <- function(...) ggplot2::scale_fill_viridis_d(..., option = "viridis")
```

# Introduction

Spatial interpolation is the process of using points with known values to estimate values at other points ([QGIS definition](https://docs.qgis.org/3.10/en/docs/gentle_gis_introduction/spatial_analysis_interpolation.html)). When asked a question like _What is the air polluants concentration over the entire city?_ and you only have access to measures at some observation stations, the answer is *spatial interpolation*. 

There are many use cases, depth maps, concentration of mineral ressources in the soil of a region, air pollution, elevation map, ...The common point is that the information of interest (altitude, concentration, ...) is known at sample points and one wants to know it at every point of a specific region. 

There exists different solutions to this spatial interpolation problem. They often consist on taking a weighted average of the sample points. In this document 3 methods are presented:

* Inverse Distance Weighting (IDW), 
* Triangular Irregular Networks (TIN), 
* Kriging

#### The mathematical problem

Before going into detail into the way these methods work, let's give a proper mathematical definition of the problem.

Let's say you have $n$ measures $z_1, \dots, z_n$ of some variable $Z$ at $n$ points in some region of space $R \subset \in \mathbb{R}^2$. You want to know the value of $Z$ at every points in that region $R$. $Z$ can be the depth and $R$ a coastal region for example. 

The ultimate goal is to define a function that for each point $(x, y) \in R$ associates a value $z \in \mathbb{R}$.

$$
(x, y) \in \mathbb{R}^2 \rightarrow z \in \mathbb{R}
$$
Finding this function is what spatial interpolation is about. To do so the only input data as your disposal are the $n$ estimates at sample points $s_1, \dots , s_n$, $s_i \in \mathbb{R}^2, i = 1 \dots n$. 

Different methods exist [see @overview] and most of them have the same general formula. The prediction $\widehat{z_0}$ for a point $s_0 \in \mathbb{R}^2$ is given by the following relation:

<a id="the-equation"></a>
$$
\widehat{z_0} = \sum_{i = 1}^{n}\lambda_i z_i
$$

The predictions are weigthed average of the values observed at sample points $s_1, \dots , s_n$. The way the weights $\lambda_1, \dots , \lambda_n$ are defined is what make each method different from the others. However weights are defined, every method follows the [Tobler's first law of geography](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography) that states that _"everything is related to everything else, but near things are more related than distant things"_.

#### The application

The methods will be explained theoritically and code will be provided to apply them on toy data. Although the aim of this document is not to compare each method, as choosing the best one depends on the data, the same example data will be used throughout the document. It consists of a gaussian random field simulated on a $[0, 100] \times [0, 100]$ grid with the `geoR::grf()` function. A random sample of 100 points are selected and will be used as the basis for the spatial interpolation. 

<a id="grf"></a>
```{r grid, out.width="100%"}
simulated_grid <- readRDS("../data/grf_spheric_01x01.rds")
set.seed(123)
sample_points <- simulated_grid[sample(1:10201, 100), ]
plot_map(simulated_grid, "The 100 sample points on the true map") +
  geom_point(
    data = sample_points,
    mapping = aes(x = x, y = y),
    size = 1
  )
```

Besides this pretty simple scenario, additional use cases will be explored for each methods. 

# Inverse Distance Weighting

The first method that we'll explore is IDW. It uses [this equation](#the-equation) to make the predictions and the weights are defined using, you guessed it, the inverse of the distance to sample points.

After explaining the how, 2 applications with R code are provided, a really basic one to illustrate the theory and a more challenging one, where barriers are added, but we'll come to that later. 

## How it works

First let's dive into the method. The weights $\lambda_1, \dots , \lambda_n$ are defined by taking the inverse of the distance between the point to predict, $s_0$, and the sample points $s_1, \dots , s_n$, at a specific power $p$. This method directly considers that the closer a point is to another the more alike they are. If $dist(a, b) = ||a-b||^2$ is the euclidean distance between points $a, b \in \mathbb{R}^2$ then for $s_0 \in R$, $\forall i = 1\dots n$ and $p \in \mathbb{N}$, the weights $\lambda_1, \dots , \lambda_n$ are defined with the following equation.

<a id="idw-weight"></a>
$$
\lambda_i = \frac{\left(dist(s_0, s_i)^p\right)^{-1}}{\left(\sum_{j=1}^{n}dist(s_0, s_j)^p\right)^{-1}}
$$

The weights are normalized by the sum of all weights so they sum to 1. This also removes the dependance from the unit (metres or km). The parameter $p$ defines the rate at which the influence of a point to another decreases. The higher $p$ is the fastest the decrease of influence is.

```{r power}
x_seq <- seq(0, 10, length.out = 11)
data <- data.table(
    x = rep(x_seq, 5),
    y = c(1 / x_seq^0, 1 / x_seq^1, 1 / x_seq^2, 1 / x_seq^3, 1 / x_seq^4),
    color = rep(as.character(0:4), each = 11)
  ) %>% 
  .[x == 3, label := paste("p =", color), by = color]
ggplot(
  data = data, 
  mapping = aes(x = x, y = y, color = color, group = color)
) + 
  geom_line(
    show.legend = FALSE
  ) + 
  scale_x_continuous(
    breaks = seq(0, 10, length.out = 6)
  ) + 
  geom_label_repel(
    mapping = aes(label = label),
    nudge_x = 1,
    show.legend = FALSE
  ) + 
  labs(
    x = "Distance",
    y = latex2exp::TeX("$1 / x^p$"),
    title = "Comparison of different power orders"
  )
```

$p$ is an hyper-parameter of the IDW method and the most adequate value should be selected. One solution to this problem will be presented in a [dedicated section](#p-choice) later.

## A first application

First let's apply IDW on our [simulated random field](#grf). Recall we have 100 points and we want to predict the value of the variable of interest $Z$ for every point of the grid $[0, 100] \times [0, 100]$.

Building the interpolated map means making a prediction for each point in the grid. Following formula 2 and 3 we need to:

1. compute the distance matrix between each point and the sample points
2. use these distances to build the weights
3. build our prediction according to equation 2 $\widehat{z_0} = \sum_{i = 1}^{n}\lambda_i z_i$

### Step by step code {#idw-step-by-step}

The process to make predictions on the grid is presented in the following code. Of course specific functions exist to do just that but let's break it piece by piece first. 

The order used here is 5, we will see later on how this value was selected.

```{r idw_base, echo = TRUE}
# Distance matrix
distance <- rdist(simulated_grid[, c("x", "y")], sample_points[, c("x", "y")])

# Weights matrix
# Take the inverse of every element of the distance matrix
weights <- apply(
  X = distance,
  MARGIN = 1:2,
  FUN = function(x) 1 / x^5
)
# Divide (FUN = "/") the weights by the row sum (STATS = rowSums(weights)) to every element of the
# weights matrix (x = weights) row wise (MARGIN = 1)
weights <- sweep(
  x = weights,
  MARGIN = 1,
  STATS = rowSums(weights),
  FUN = "/"
)

# Multiply the weights matrix with the z value of the sample points
pred <- weights %*% sample_points$z
```

Those lines of code are the only ones needed to have an estimated $Z$ value at each location of the grid $[0, 100] \times [0, 100]$. The result, `pred`, is a table with $101 \times 101 = 10201$ values, one for each point of the grid. The following figure displays the predicted map alongside the true data.

```{r idw_pred_map}
type_lables <- c("IDW map", "Real map")
names(type_lables) <- c("pred", "true")
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "true" = simulated_grid$z) %>%
  merge(
    y = sample_points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
plot_map("Prediction with IDW and real data") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = type_lables)
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

The map is globally the same (if you're not too exigent) but much smoother than the real one. It displays some "bubbles" around the sample points which is of course coherent with the fact that each point has a strong influence only on its immediate neighborhood (recall $p$ was set to 5).

### How to chose $p$? {#p-choice}

The order $p$ was set to 5. Why not 3 or 6 or even 10? The choice of $p$ depends on the data you have and one should always try to find the most appropriate value. One way to do so is to evaluate the error of prediction using a leave-one-out estimation. For each point in the sample a prediction is made using all the other points. The estimation of the error is then the average error observed. 

More specifically if we have $n$ points $s_1, \dots, s_n$, the leave-one-out error estimation is computed as follows:

$$
\begin{equation}
\begin{split}
E_{LOO} &= \sqrt{\frac{1}{n} \sum_{i=1}^{n}(z_i - \widehat{z_i})^2} \\
\widehat{z_i} &= \sum_{j=1, j \ne i}^{n}\lambda_j z_j
\end{split}
\end{equation}
$$

The weights are computed as defined in [equation 3](#idw-weight). This error is computed for different values of $p$ and the value of $p$ that minimizes the error is to be chosen. 

The following piece of code illustrates this method. The error is estimated for $p = 1\dots 20$.

```{r loo, echo = TRUE}
rmse_errors <- sapply(
  X = 1:20, # Values of p
  FUN = function(p) { # Compute the RMSE estimation for each value of p
    pred_sample <- numeric(length = 100)
    for (i in length(pred_sample)) {
      # Predict point i and remove it from sample points
      distance <- rdist(sample_points[i, c("x", "y")], sample_points[-i, c("x", "y")])
      weights <- apply(
        X = distance,
        MARGIN = 1:2,
        FUN = function(x) 1 / x^p
      )
      weights <- sweep(
        x = weights,
        MARGIN = 1,
        STATS = rowSums(weights),
        FUN = "/"
      )
      pred_sample[i] <- weights %*% sample_points[-i, ]$z %>% as.numeric()
    }
    return(sqrt(sum((pred_sample - sample_points$z)^2) / nrow(sample_points)))
  }
)
```


```{r loo_plot}
ggplot(
  data = data.frame(x = 1:20, y = rmse_errors),
  mapping = aes(x = x, y = y)
) +
  geom_line() +
  labs(
    x = "Power order",
    y = "Leave-one-out RMSE",
    title = "RMSE for different orders p"
  )
```


The RMSE (estimated with leave-one-out) first decreases a lot and reach a plateau around 5. The actual best value is `r which.min(rmse_errors)` but from this plot one can see that choosing 5 or 20 does not make a huge difference.


### Of course an idw function exists

The package `gstat` has an `idw()` function that does exactly what we did. It takes 5 arguments: 

* `formula` a formula giving the name of the target variable. For IDW the right hand side is 1, meaning no independant variables.
* `locations` a formula giving the names of the coordinates variables.
* `data` the name of the dataset containing the variables defined in `formula` and `locations`.
* `newdata` a table with the coordinates where to make predictions.
* `idp` the power order to use.

```{r idw, echo = TRUE, eval = FALSE}
pred_gstat <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = sample_points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5
)
```

```{r idw2, echo = FALSE}
pred_gstat <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = sample_points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5,
  debug.level = 0
)
```

As a sanity check let's compare the map obtained from the code of the [previous section](#idw-step-by-step) and the one with `gstat::idw()`.

```{r idw_sanity}
names(pred_gstat) <- c("x", "y", "z", "var")
data.table(simulated_grid[, c("x", "y")], "idw_base" = pred[, 1], "idw_gstat" = pred_gstat$z) %>%
  merge(
    y = sample_points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(idw_base), idw_base := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Step by step and gstat IDW produce the same maps") +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("idw_base" = "Step by step", "idw_gstat" = "gstat::idw()"))) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

## Adding some spatial constraints

```{r barriers_data}
barriers <- readRDS("../data/barriers.rds")
points <- readRDS("../data/points.rds")
```

So far we considered that each sample point influences every point of the region (even if this influence could be small depending on the distance). In the presence of barriers (obstacles blocking the underlying physical process, causing linear discontinuity in the surface) some points can have no influence at all. When interpolated a value in a coastal region, we often encounter such barriers, (islands, port docks, ...).

To account for the presence of such barriers the interpolation algorithm must be modified. One can no longer use euclidean distance between all points to make a prediction. A first solution is to remove sample points that are not in the _line of sight_ of the point to predict.

```{r line-of-sight, fig.cap = "Source = [ArcGIS blog on esri.com](https://www.esri.com/arcgis-blog/products/analytics/analytics/interpolation-in-the-presence-of-barriers/)"}
include_graphics("images/idw_barriers.png")
```

Whenever a sample point is not in line of sight of the point to predict then its value is not used (its weight is set to 0) for the interpolation.

Another method, called Inverse Path Distance Weighting (IPDW) could also be used [see @ipdw]. It creates a cost surface and shortest path to define the distance between points. [This vignette](https://cran.r-project.org/web/packages/ipdw/vignettes/ipdw2.html), from the `ipdw` package [see @ipdw-package] explained the process.

A different toy data is used in this section.

```{r barrier_data_plot}
ggplot() + 
  geom_sf(data = barriers) + 
  geom_point(
    data = points,
    mapping = aes(x = x, y = y)
  ) + 
  geom_rect(
    mapping = aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1),
    color = "grey20",
    fill = NA
  ) + 
  # expand_limits(
  #     x = c(-0.1, 1.1),
  #     y = c(-0.1, 1.1)
  #   ) +
  labs(
    title = "Region to interpolate with sample points and barriers"
  ) + 
  theme(
    panel.grid = element_blank(),
    axis.line = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  )
```


We detail here how the first solution could be implemented. The idea is to update the weights matrix (as defined in [this section](#idw-step-by-step)) by setting to 0 weights between points that are not in _line of sight_. This piece of code is quite slow and only serves as explanatory purpose.

```{r constained_idw, echo = TRUE, eval = FALSE}
# First compute the distance matrix between all points of the grid and all sample points
distance <- rdist(simulated_grid[, c("x", "y")], points[, c("x", "y")])

# Define the power order
p <- 5

# Take the inverse of every element
weights <- apply(
  X = distance,
  MARGIN = 1:2,
  FUN = function(x) 1 / x^p
)

#### The weights update

# Remove connection that are not in "line of sight". For each pair of point build a line and check
# wether it crosses a barrier
to_remove <- matrix(FALSE, nrow = nrow(weights), ncol = ncol(weights))
for (i in seq_len(nrow(simulated_grid))) {
  for (j in seq_len(nrow(points))) {
    # Build the line between a sample point and the point to predict
    to_remove[i, j] <- rbind(as.numeric(points[j, c("x", "y")]), as.numeric(simulated_grid[i, c("x", "y")])) %>%
      st_linestring() %>%
      # Check whether it crosses a barrier
      st_intersects(barriers) %>%
      lengths() > 0
  }
}
weights[to_remove] <- 0

####

# Divide (FUN = "/") the weights by the row sum (STATS = rowSums(weights)) to every element of the
# weights matrix (x = weights) row wise (MARGIN = 1)
weights <- sweep(
  x = weights,
  MARGIN = 1,
  STATS = rowSums(weights),
  FUN = "/"
)

# Multiply the weights matrix with the z value of the sample points
pred <- weights %*% points$z
```

The resulting map is given in the following figure along with the map obained with regular IDW. 

```{r idw_constained_plot}
# Interpolation with IDW
pred_idw <- idw(
  formula = z ~ 1,
  locations = ~ x + y,
  data = points,
  newdata = simulated_grid[, c("x", "y")],
  idp = 5
)
names(pred_idw) <- c("x", "y", "z", "var")
# Load results from constained IDW
pred <- readRDS("../output/pred_idw_barriers.rds")
# Plot the result alongside the IDW prediction
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "idw" = pred_idw$z) %>%
  merge(
    y = points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := NULL] %>%
  melt(
    id.vars = c("x", "y"),
    variable.name = "type",
    value.name = "z"
  ) %>%
  plot_map("Prediction with IDW and constrained IDW") +
  geom_sf(data = barriers) +
  facet_grid(
    cols = vars(type),
    labeller = labeller(type = c("pred" = "Constrained IDW", "idw" = "Regular IDW"))
  ) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(2.5, "lines")
  )
```

The impact of taking the barrier into account clearly appears. The interpolated values south of the bottom left barrier have lower values in the constrained IDW case. The barrier blocks the influence of the point north of that barrier to go beyond it. The difference _Constrained - Regular IDW_ is plotted on this figure.

```{r barrier_reg_plot}
data.table(simulated_grid[, c("x", "y")], "pred" = pred[, 1], "idw" = pred_idw$z) %>%
  merge(
    y = points,
    by = c("x", "y"),
    all.x = TRUE
  ) %>%
  .[is.na(pred), pred := z] %>%
  .[, z := pred - idw] %>%
  plot_map("Difference between the constrained IDW and the regular one") + 
  geom_sf(data = barriers)
```

We clearly see that the barriers prevent the high values located in the north-west part of the region to extend beyond the barriers. 

# Triangulated irregular network

[Delaunay traingulation](http://www.cs.uu.nl/geobook/interpolation.pdf)

# Kriging

Kriging is another method used to perform spatial interpolation. As IDW it uses the  [second formula](#the-equation) $\widehat{z_0} = \sum_{i = 1}^{n}\lambda_i z_i$ to make predictions. The way the weights are defined is what makes this methods specific. 

Before explaining the method we need to define some concepts: random field and stationarity. A random field is simply a set of random variable. We consider each value $z_1, \dots, z_n$ to be observations of the random variables $Z_1, \dots, Z_n$. $Z(s)$ is a random function, or random field. $s$ represents a position in space. 

There are 3 king of stationarity: the strict stationarity, the second-order stationarity and the intrinsic stationarity.

#### The strict stationarity

It states that the joint distribution of $Z(s_i)$ is the same as the one of $Z(s_i + h)$ where $h$ indicates a translation. 

#### The second-order stationarity

It is weaker than the first one as it only requires the mean and variance to be invariant by translation. That is $\forall s,  \mathbb{E}[Z(s)] = \mathbb{E}[Z(s+h)] = m$ and $V(Z(s+h)) = V(Z(s)) = \sigma^2$. The covariance function only depends on $h$ and therefore the function $C$ can be defined as $Cov(Z(s+h), Z(h)) = C(h)$.

#### The intrinsic stationarity

The condition is no longer on the random field itself but on the increments $Z(s+h) - Z(h)$. The expectation should be 0, $\mathbb{E}[Z(s+h) - Z(h)] = 0$ and the variance only depends on $h$. As previously we can define a function $\gamma$, called the semi-variogram, as $V(Z(s+h), Z(s)) = \mathbb{E}[(Z(s+h) - Z(h))^2] = 2\gamma(h)$.

http://www.u.arizona.edu/~donaldm/homepage/my_papers/Mathematical%20Geology%2021%201989%20347-363.pdf
https://www.aspexit.com/en/fundamental-assumptions-of-the-variogram-second-order-stationarity-intrinsic-stationarity-what-is-this-all-about/

Now that some basic concepts of stationarity have been defined, let's dive into the kriging method. We will be defining ordinary Kriging.

## How it works

We want to find the weights $\lambda_i$ required to predict the value $z_0$. We want our estimate to be unbiased, on average we want our predictions to be exact. In other words we want 

$$
\begin{equation}
\begin{split}
& \mathbb{E}[\widehat{z_0} - z_0] = 0 \\
\Leftrightarrow \ & \mathbb{E}\left[\sum_{i = 1}^{n}\lambda_i z_i - z_0\right] = 0 \\
\Leftrightarrow \ & \sum_{i = 1}^{n}\lambda_i \mathbb{E}[z_i] - \mathbb{E}[z_0] = 0 \\
\Leftrightarrow^* \ & \sum_{i = 1}^{n}\lambda_i m - m = 0 \\
\Leftrightarrow \ & \sum_{i = 1}^{n}\lambda_i = 1
\end{split}
\end{equation}
$$

To be able to go from line 3 to 4 we need the relation $\mathbb{E}[z_i] = \mathbb{E}[z_0] = m$. This is the second-order stationarity assumption that we suppose our random field $Z(s)$ fulfills. stationarity is more like a model assumption than an hypothesis to be tested. It is a way to simplify the real world and as always in statistics such assumptions can be removed, leading to more complex models which we'll see later in this document. 

In order for our estimator to be a good one we also need its variance to be as low as possible. 


# Help

$$
\begin{equation} 
\begin{split}
\mathrm{Var}(\hat{\beta}) & =\mathrm{Var}((X'X)^{-1}X'y)\\
 & =(X'X)^{-1}X'\mathrm{Var}(y)((X'X)^{-1}X')'\\
 & =(X'X)^{-1}X'\mathrm{Var}(y)X(X'X)^{-1}\\
 & =(X'X)^{-1}X'\sigma^{2}IX(X'X)^{-1}\\
 & =(X'X)^{-1}\sigma^{2}
\end{split}
\end{equation} 
$$

# References